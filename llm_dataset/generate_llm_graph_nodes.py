"""
This script generates a graph dataset with nodes representing topics generated by a Large Language Model (LLM).
The generated nodes are then embedded using a sentence-transformer model and saved as a PyTorch Geometric data object.
The script is designed to create two types of datasets: one with topics related to a specific domain (e.g., sustainable development)
and another with topics from a different domain (e.g., medicine and biology). This allows for the evaluation of the GNN's
generalization capabilities.

The script performs the following steps:
1.  **Defines root topics**: Two lists of root topics are defined, one for the domain of interest and one for a non-related domain.
2.  **Initializes LLM**: A Hugging Face LLM is initialized for generating related topics.
3.  **Generates nodes**: For each root topic, the LLM is prompted to generate a list of related arguments or topics.
4.  **Embeds nodes**: The generated node labels (topics) are embedded into a high-dimensional vector space using a sentence-transformer model.
5.  **Saves graph tensor**: The node embeddings and an empty edge index are stored in a PyTorch Geometric `Data` object, which is then saved to a file.
"""

import json
import time
from sentence_transformers import SentenceTransformer
import torch
from tqdm import tqdm
import sys
import logging
import os
from torch_geometric.data import Data
import random

import gc
import transformers
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from custom_logger.logger_config import get_logger

log: logging.Logger = get_logger(name=__name__)

# Folder to save LLM generated graph
TENSOR_GRAPH_OUTDIR = "/home/bruno/Documents/GitHub/social-media-nlp/llm_dataset/only_node_datasets"

# A list of root topics related to sustainable development. These topics are used to prompt the LLM to generate a domain-specific dataset.
ROOT_DOMAIN_TOPICS: list[str] = [
    "Sustainable Development Goals 1: End poverty in all its forms everywhere",
    "Sustainable Development Goals 2: End hunger, achieve food security and improved nutrition and promote sustainable agriculture",
    "Sustainable Development Goals 3: Ensure healthy lives and promote well-being for all at all ages",
    "Sustainable Development Goals 4: Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all",
    "Sustainable Development Goals 5: Achieve gender equality and empower all women and girls",
    "Sustainable Development Goals 6: Ensure availability and sustainable management of water and sanitation for all",
    "Sustainable Development Goals 7: Ensure access to affordable, reliable, sustainable and modern energy for all",
    "Sustainable Development Goals 8: Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all",
    "Sustainable Development Goals 9: Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation",
    "Sustainable Development Goals 10: Reduce inequality within and among countries"
]

# A list of root topics related to medicine and biology. These topics are used to prompt the LLM to generate a non-domain-specific dataset for evaluating the GNN's generalization.
ROOT_NON_DOMAIN_TOPICS: list[str] = [
    "Genetic Engineering and CRISPR Technology: Explore the revolutionary potential of gene editing tools like CRISPR-Cas9 in treating genetic diseases, developing new therapies, and even enhancing human capabilities",
    "Immunotherapy and Cancer Treatment: Investigate the latest advancements in harnessing the immune system to fight cancer, including checkpoint inhibitors, CAR T-cell therapy, and personalized cancer vaccines",
    "Neurodegenerative Diseases: Delve into the complexities of Alzheimer's disease, Parkinson's disease, and other neurological disorders, examining the underlying causes, potential treatments, and the challenges of developing effective therapies",
    "Microbiome and Human Health: Discover the intricate world of the human microbiome the trillions of bacteria, fungi, and other microorganisms living in our bodies and its profound impact on digestion, immunity, mental health, and overall well-being",
    "Stem Cell Research and Regenerative Medicine: Explore the therapeutic potential of stem cells in repairing damaged tissues and organs, treating diseases like diabetes and spinal cord injuries, and even growing new organs for transplantation",
    "Artificial Intelligence in Healthcare: Examine the growing role of AI in medicine, from diagnosing diseases and analyzing medical images to developing new drugs and personalizing treatment plans",
    "Epidemiology and Global Health: Study the patterns, causes, and control of diseases in populations, including infectious diseases like COVID-19, as well as chronic conditions like heart disease and cancer, with a focus on improving global health outcomes",
    "Mental Health and Neuroscience: Investigate the biological basis of mental health disorders, such as depression, anxiety, and schizophrenia, and explore new approaches to diagnosis, treatment, and prevention",
    "Aging and Longevity: Explore the biological processes of aging and the factors that contribute to healthy aging and longevity, including genetics, lifestyle, and environmental influences",
    "Biotechnology and Drug Development: Learn about the cutting-edge technologies used to develop new drugs and therapies, including monoclonal antibodies, gene therapy, and personalized medicine approaches",
]

# The Hugging Face model name for the LLM and the sentence-transformer model for embedding.
HF_MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"
HF_EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"

# The system prompt used to instruct the LLM to be concise and informative.
SYSTEM_PROMPT = """You are a large language model, trained to be informative and concise.
Do not provide explanations or elaborations unless explicitly requested.
Focus on delivering the most direct and accurate answer to the user's query."""

# The number of nodes (topics) to be generated by the LLM for each root topic.
NUM_GEN_NODES = 10

def __get_prompt(root_topic: str, num_gen_nodes: int = NUM_GEN_NODES) -> str:
    """
    Generates a prompt for the LLM to generate a list of related topics.

    Args:
        root_topic (str): The root topic for which related topics are to be generated.
        num_gen_nodes (int, optional): The number of related topics to generate. Defaults to NUM_GEN_NODES.

    Returns:
        str: The generated prompt string.
    """
    return f"Generate a JSON list of {num_gen_nodes} arguments related to {root_topic}." + \
        "\n\nExample of response structure: [\"apple\", \"banana\", \"cherry\"]" + \
        "\nAvoid creating nested JSON object, the generated JSON object list must have only one level."

def __get_gen_nodes(llm_out: str) -> list[str]:
    """
    Parses the LLM's output to extract the generated list of nodes.

    Args:
        llm_out (str): The raw output string from the LLM.

    Returns:
        list[str]: A list of node labels (topics).
    """
    start_idx = llm_out.index("[")
    end_idx = llm_out.index("]")
    json_str = llm_out[start_idx:end_idx + 1]
    json_loaded = json.loads(json_str)
    
    nodes: list[str] = []
    for item in json_loaded:
        nodes.append(item)
    
    log.info(f"Nodes generated from LLM: {nodes}")
    return nodes

def __initialize_llm(model_name: str = HF_MODEL_NAME) -> tuple[AutoModelForCausalLM, AutoTokenizer]:
    """
    Initializes and returns the Hugging Face Causal Language Model and its tokenizer.

    Args:
        model_name (str, optional): The name of the Hugging Face model to initialize. Defaults to HF_MODEL_NAME.

    Returns:
        tuple[AutoModelForCausalLM, AutoTokenizer]: A tuple containing the initialized model and tokenizer.
    """
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=False,
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name, 
        quantization_config=bnb_config, 
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    log.info(f"Initialized LLM: {model_name}")
    return model, tokenizer

def __generate_llm(
    model: AutoModelForCausalLM, 
    tokenizer: AutoTokenizer, 
    prompt: str,
    system_prompt: str = SYSTEM_PROMPT
) -> str:
    """
    Generates a response from the LLM given a prompt.

    Args:
        model (AutoModelForCausalLM): The initialized LLM.
        tokenizer (AutoTokenizer): The tokenizer for the LLM.
        prompt (str): The user prompt to the LLM.
        system_prompt (str, optional): The system prompt to guide the LLM's behavior. Defaults to SYSTEM_PROMPT.

    Returns:
        str: The LLM's generated response.
    """
    pipe = transformers.pipeline(
        model=model,
        tokenizer=tokenizer,
        return_full_text=False,
        task="text-generation",
        max_new_tokens=1024,
        temperature=0.7,
        do_sample=True,
        top_p=0.95,
        top_k=100
    )
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt},
    ]
    gen_seqs = pipe(messages)
    llm_response = "".join([seq["generated_text"] for seq in gen_seqs])
    log.info(f"Generate LLM response of length {len(llm_response)}")
    
    del pipe
    gc.collect()
    torch.cuda.empty_cache()
    return llm_response

def __generate_nodes(root_topic_list: list[str] = ROOT_DOMAIN_TOPICS) -> list[str]:
    """
    Generates a list of node labels by prompting the LLM with a list of root topics.

    Args:
        root_topic_list (list[str], optional): A list of root topics to generate nodes from. Defaults to ROOT_DOMAIN_TOPICS.

    Returns:
        list[str]: A list of all generated node labels.
    """
    all_nodes:list[str] = []
    model, tokenizer = __initialize_llm()
    for root_topic in tqdm(root_topic_list, desc="Generating nodes LLM"):
        prompt = __get_prompt(root_topic)
        llm_response = __generate_llm(model, tokenizer, prompt)
        nodes = __get_gen_nodes(llm_response)
        all_nodes.extend(nodes)
    
    # Destroy LLM model and clean VRAM
    del model, tokenizer
    gc.collect()
    torch.cuda.empty_cache()
    
    return all_nodes

def __save_graph_tensor(
    node_list: list[str], 
    description: str,
    save_path: str = TENSOR_GRAPH_OUTDIR,
    hf_embedding_model: str = HF_EMBEDDING_MODEL_NAME
) -> None:
    """
    Embeds a list of node labels and saves them as a PyTorch Geometric graph tensor.

    Args:
        node_list (list[str]): The list of node labels to embed and save.
        description (str): A description to include in the filename of the saved tensor.
        save_path (str, optional): The directory to save the tensor file. Defaults to TENSOR_GRAPH_OUTDIR.
        hf_embedding_model (str, optional): The name of the sentence-transformer model to use for embedding. Defaults to HF_EMBEDDING_MODEL_NAME.
    """
    embedding_model: SentenceTransformer = SentenceTransformer(hf_embedding_model)
    log.info(f"Loaded embedding model {hf_embedding_model}")
    
    nodes_idx_map: dict[int, str] = {}
    nodes_embed_ord: list[torch.Tensor] = []
    
    random.shuffle(node_list)
    for node_label in tqdm(node_list, desc="Parsing LMM generated nodes"):
        nodes_idx_map[len(nodes_idx_map)] = node_label
        node_embed = embedding_model.encode(node_label)
        node_embed_tensor = torch.tensor(node_embed, dtype=torch.float)
        nodes_embed_ord.append(node_embed_tensor)
    
    x = torch.stack(nodes_embed_ord)
    edge_index = torch.tensor([[], []], dtype=torch.int64)
    data = Data(x=x, edge_index=edge_index)
    tensor_to_save = {
        "data": data,
        "nodes_idx_map": nodes_idx_map
    }
    
    tesor_file_name = f"llm_gen_graph_{description}_{len(node_list)}_{time.strftime('%Y%m%d-%H%M%S')}.pt"
    tensor_file_path = os.path.join(save_path, tesor_file_name)
    torch.save(obj=tensor_to_save, f=tensor_file_path)
    log.info(f"Graph of {len(node_list)} nodes saved to {tensor_file_path}")

if __name__ == "__main__":
    # This script block demonstrates how to generate and save a graph dataset.
    # It generates nodes based on the ROOT_DOMAIN_TOPICS and saves them as a tensor file.
    list_nodes = __generate_nodes(root_topic_list=ROOT_DOMAIN_TOPICS)
    __save_graph_tensor(list_nodes, description="domain-related")
