import torch
from torch import nn

# SwiGLU activation function torch class
class SwiGLU(nn.Module):
    def __init__(self, dimension:int):
        super().__init__()
        self.linear_1 = nn.Linear(dimension,dimension)
        self.linear_2 = nn.Linear(dimension,dimension)

    def forward(self, x):
        output = self.linear_1(x)
        swish = output * torch.sigmoid(output)
        swiglu = swish * self.linear_2(x)

        return swiglu