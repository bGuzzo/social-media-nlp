\begin{thebibliography}{6}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin2019bertpretrainingdeepbidirectional}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding, 2019.
\newblock URL \url{https://arxiv.org/abs/1810.04805}.

\bibitem[Kipf and
  Welling(2017)]{kipf2017semisupervisedclassificationgraphconvolutional}
Thomas~N. Kipf and Max Welling.
\newblock Semi-supervised classification with graph convolutional networks,
  2017.
\newblock URL \url{https://arxiv.org/abs/1609.02907}.

\bibitem[Reimers and Gurevych(2019)]{reimers-2019-sentence-bert}
Nils Reimers and Iryna Gurevych.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics, 11
  2019.
\newblock URL \url{https://arxiv.org/abs/1908.10084}.

\bibitem[Vaswani et~al.(2023)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2023attentionneed}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need, 2023.
\newblock URL \url{https://arxiv.org/abs/1706.03762}.

\bibitem[Veličković et~al.(2018)Veličković, Cucurull, Casanova, Romero,
  Liò, and Bengio]{veličković2018graphattentionnetworks}
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
  Liò, and Yoshua Bengio.
\newblock Graph attention networks, 2018.
\newblock URL \url{https://arxiv.org/abs/1710.10903}.

\bibitem[Wang et~al.(2020)Wang, Wei, Dong, Bao, Yang, and
  Zhou]{wang2020minilmdeepselfattentiondistillation}
Wenhui Wang, Furu Wei, Li~Dong, Hangbo Bao, Nan Yang, and Ming Zhou.
\newblock Minilm: Deep self-attention distillation for task-agnostic
  compression of pre-trained transformers, 2020.
\newblock URL \url{https://arxiv.org/abs/2002.10957}.

\end{thebibliography}
