\documentclass{article}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{algpseudocode}
\usepackage{graphicx}

\title{Algorithm for Extracting a Graph from Wikipedia Links}
\author{}
\date{}

\begin{document}
	
	\maketitle
	
	\section{Abstract}
	
	This document describes an algorithm for extracting a graph from Wikipedia links, starting from a root article. The algorithm uses a breadth-first search approach to crawl the Wikipedia links and build the graph, where nodes represent Wikipedia pages and edges represent links between them.  The algorithm also includes error handling for cases such as non-existent pages and disambiguation errors.
	
	\section{Introduction}
	
	The extraction of knowledge graphs from Wikipedia is a crucial task in various domains, including natural language processing, information retrieval, and data mining. This document details a specific algorithm designed for this purpose. It leverages the \texttt{wikipedia} and \texttt{networkx} Python libraries to retrieve page content and manage the graph structure, respectively. The algorithm performs a breadth-first search, starting from a specified root article, to explore linked Wikipedia pages and construct a graph representation of their relationships.
	
	\section{Algorithm Description}
	
	The algorithm, named \texttt{extract\_graph}, takes the title of a root Wikipedia article and an optional maximum number of nodes as input. It returns a NetworkX graph object representing the extracted knowledge graph. The algorithm utilizes a helper function \texttt{\_\_crawl} which implements the breadth-first search.
	
	\subsection{Pseudo-code}
	
	\begin{algorithmic}[1]
		\Procedure{extract\_graph}{root\_article\_title, max\_nodes}
		\State \texttt{\_\_check\_title(root\_article\_title)} \Comment{Check if root article exists}
		\State \texttt{graph} $\leftarrow$ \texttt{nx.Graph()}
		\State \texttt{graph.add\_node(root\_article\_title)}
		\State \texttt{visited} $\leftarrow$ \texttt{\{root\_article\_title\}}
		\State \texttt{queue} $\leftarrow$ \texttt{[root\_article\_title]}
		\State \texttt{\_\_crawl(graph, queue, visited, max\_nodes)}
		\If{\texttt{not nx.is\_connected(graph)}}
		\State \textbf{raise} Exception("Graph is not connected")
		\EndIf
		\State \textbf{return} \texttt{graph}
		\EndProcedure
		
		\Procedure{\_\_crawl}{graph, queue, visited, max\_nodes}
		\If{\texttt{not queue}}
		\State \textbf{return}
		\EndIf
		\If{\texttt{len(graph.nodes)} $\geq$ \texttt{max\_nodes}}
		\State \textbf{return}
		\EndIf
		\State \texttt{page\_title} $\leftarrow$ \texttt{queue.pop(0)}
		\State \textbf{Try}
		\State \texttt{page} $\leftarrow$ \texttt{\_\_check\_title(page\_title)}
		\State \textbf{Catch(Exception)}
		\State \textbf{return} \Comment{Skip this branch}
		\State \textbf{EndTry}
		\For{\texttt{link\_title} \textbf{in} \texttt{page.links}}
		\If{\texttt{len(graph.nodes)} $\geq$ \texttt{max\_nodes}}
		\State \textbf{return}
		\EndIf
		\If{\texttt{link\_title} \textbf{in} \texttt{visited}}
		\State \texttt{\_\_add\_edge(graph, page\_title, link\_title)}
		\Else
		\State \texttt{visited.add(link\_title)}
		\State \texttt{queue.append(link\_title)}
		\State \texttt{graph.add\_node(link\_title)}
		\State \texttt{\_\_add\_edge(graph, page\_title, link\_title)}
		\EndIf
		\EndFor
		\State \texttt{\_\_crawl(graph, queue, visited, max\_nodes)}
		\EndProcedure
		
		\Procedure{\_\_add\_edge}{graph, page\_src, page\_dst}
		\If{\texttt{page\_src == page\_dst}}
		\State \textbf{return} \Comment{Avoid self-links}
		\EndIf
		\If{\texttt{graph.has\_edge(page\_src, page\_dst)} \textbf{or} \texttt{graph.has\_edge(page\_dst, page\_src)}}
		\State \textbf{return} \Comment{Edge already present}
		\EndIf
		\State \texttt{graph.add\_edge(page\_src, page\_dst)}
		\EndProcedure
		
		\Procedure{\_\_check\_title}{page\_title}
		\State \textbf{Try}
		\State \texttt{page} $\leftarrow$ \texttt{wikipedia.page(page\_title)}
		\State \textbf{Catch(wikipedia.exceptions.PageError)}
		\State \textbf{raise} Exception("Page not found")
		\State \textbf{Catch(wikipedia.exceptions.DisambiguationError)}
		\State \textbf{raise} Exception("Disambiguation Error")
		\State \textbf{EndTry}
		\State \textbf{return} \texttt{page}
		\EndProcedure
	\end{algorithmic}
	
	\subsection{Implementation Details}
	
	The implementation uses the \texttt{wikipedia} library for fetching page content and the \texttt{networkx} library for graph manipulation.  The \texttt{\_\_check\_title} function handles potential exceptions during page retrieval. The \texttt{\_\_add\_edge} function ensures no duplicate or self-referential edges are added.  The \texttt{json\_dump} function provides a way to serialize the graph into JSON format. The example usage demonstrates how to extract and visualize the graph using \texttt{matplotlib}.
	
	\section{Conclusion}
	
	This document presented a detailed algorithm for extracting knowledge graphs from Wikipedia. The algorithm uses a breadth-first search strategy and incorporates error handling to ensure robustness. The generated graph can be further analyzed and visualized for various applications.
	
\end{document}