\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\begin{document}
	
	\title{Graph Attention Networks: A Detailed Explanation}
	\author{}
	\date{}
	\maketitle
	
	\section{Introduction}
	
	Graph Attention Networks (GATs) are a class of neural network architectures designed to operate on graph-structured data. They leverage a self-attention mechanism to dynamically learn the importance of neighboring nodes, enabling effective feature aggregation while being independent of the underlying graph structure and allowing inductive learning.
	
	\section{Graph Attentional Layer}
	
	The fundamental building block of a GAT is the \textit{graph attentional layer}. Given a graph with $N$ nodes, each represented by a feature vector $\mathbf{h}_i \in \mathbb{R}^F$ ($i \in \{1, \dots, N\}$), the objective is to produce a new set of node features, $\mathbf{h}_i' \in \mathbb{R}^{F'}$.
	
	\subsection{Feature Transformation}
	Initially, each node's feature vector is linearly transformed:
	\begin{equation}
		\hat{\mathbf{h}}_i = \mathbf{W} \mathbf{h}_i,
	\end{equation}
	where $\mathbf{W} \in \mathbb{R}^{F' \times F}$ is a learnable weight matrix.
	
	\subsection{Attention Mechanism}
	Next, an attention mechanism is employed to compute the importance of node $j$'s features to node $i$. This is achieved through a shared attentional mechanism $a: \mathbb{R}^{F'} \times \mathbb{R}^{F'} \rightarrow \mathbb{R}$:
	\begin{equation}
		e_{ij} = a(\hat{\mathbf{h}}_i, \hat{\mathbf{h}}_j).
	\end{equation}
	In practice, $a$ is often implemented as a single-layer feedforward neural network with parameters $\mathbf{a} \in \mathbb{R}^{2F'}$:
	\begin{equation}
		e_{ij} = \text{LeakyReLU}\left( \mathbf{a}^T [\hat{\mathbf{h}}_i || \hat{\mathbf{h}}_j] \right),
	\end{equation}
	where $||$ denotes concatenation and $\text{LeakyReLU}$ is a nonlinear activation function.
	
	\subsection{Masked Attention}
	To incorporate graph structure, the attention mechanism is masked. The coefficients $e_{ij}$ are only computed for nodes $j \in \mathcal{N}_i$, where $\mathcal{N}_i$ is the set of neighbors of node $i$ (including $i$ itself).
	
	\subsection{Normalization}
	The coefficients are then normalized using a softmax function over the neighborhood of each node:
	\begin{equation}
		\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i} \exp(e_{ik})}.
	\end{equation}
	
	\subsection{Feature Aggregation}
	Finally, the new feature vector for node $i$ is calculated by aggregating the transformed features of its neighbors, weighted by the normalized attention coefficients:
	\begin{equation}
		\mathbf{h}'_i = \sigma \left( \sum_{j \in \mathcal{N}_i} \alpha_{ij} \hat{\mathbf{h}}_j \right),
	\end{equation}
	where $\sigma$ is a nonlinear activation function.
	
	\section{Multi-Head Attention}
	To stabilize the learning process and capture different aspects of node relationships, multi-head attention is utilized.  The operation of the graph attention layer is performed $K$ times in parallel, each with a separate transformation and attention parameters $\mathbf{W}^k$ and $\mathbf{a}^k$ (respectively). The output of each head is a new node feature vector, $\mathbf{h}_i^{'k}$.
	
	\subsection{Concatenation}
	The output from the different heads can be concatenated together:
	\begin{equation}
		\mathbf{h}'_i = \underset{k=1}{\overset{K}{\parallel}} \sigma \left( \sum_{j \in \mathcal{N}_i} \alpha_{ij}^k \mathbf{\hat{h}}_j^k \right).
	\end{equation}
	In this case, the final output feature vector will have the dimension $KF'$.
	
	\subsection{Averaging}
	Alternatively, for the final layers, the outputs can be averaged followed by the final activation:
	\begin{equation}
		\mathbf{h}'_i = \sigma\left(\frac{1}{K} \sum_{k=1}^K \left( \sum_{j \in \mathcal{N}_i} \alpha_{ij}^k \mathbf{\hat{h}}_j^k \right) \right).
	\end{equation}
	Here, the final output feature vector has the dimension $F'$.
	
	
	\section{Conclusion}
	GATs, with their dynamic attention mechanism, provide a powerful tool for processing graph-structured data. Multi-head attention further enhances the model's ability to learn diverse and robust feature representations, enabling state-of-the-art performance on various graph-based tasks, especially allowing an inductive learning process where a model can generalise over unseeen graphs. The locality and parallelizability of attention calculation make it a computationally effective approach to graph feature learning.
	
\end{document}