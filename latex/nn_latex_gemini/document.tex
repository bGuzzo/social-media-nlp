\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning, fit, calc}

\begin{document}
	\section*{4.1 Network Architecture In Detail}
	
	Our network architecture for link prediction on graph structured data is composed by the \texttt{GatModelV1} class and it is built around a custom \texttt{DeepGATBlockV1} module.
	The complete architecture is designed to process graph data, and performs node embeddings, multi-layered graph attention, and link prediction through a dot product decoder.
	
	\subsection*{Overall Model Architecture}
	
	The \texttt{GatModelV1} model consists of the following components:
	\begin{enumerate}
		\item An input dropout layer to regularize input features.
		\item An input linear layer, $L_{in}$, that projects the input features into the hidden feature space.
		\item A \texttt{DeepGATBlockV1}, which applies multiple levels of GAT layers, dropout, normalization, and Feed-Forward layers, as we will discuss in next section.
		\item An output linear layer, $L_{out}$, which transforms the processed hidden features into the final embedding space.
	\end{enumerate}
	
	\paragraph*{Mathematical Representation:}
	
	The encoding process can be mathematically expressed as follows:
	\begin{enumerate}
		\item Given an input node feature matrix $X \in \mathbb{R}^{N \times F_{in}}$, where $N$ is the number of nodes and $F_{in}$ is the number of input features.
		\item The input features undergo a linear transformation and dropout: $X' = Dropout(L_{in}(X))$, where $L_{in} : \mathbb{R}^{F_{in}} \to \mathbb{R}^{H}$, and $H$ is the number of hidden channels.
		\item The transformed features, $X'$, are processed by the \texttt{DeepGATBlockV1}: $Z = \text{DeepGATBlockV1}(X', E)$, where $E$ represents the edge index of the graph.
		\item Finally, a linear projection is applied to the GAT block output: $Z_{out} = L_{out}(Z)$, where $L_{out}: \mathbb{R}^H \to \mathbb{R}^{F_{out}}$ and $F_{out}$ is the number of output channels.
	\end{enumerate}
	The decoder then computes the logits of edge existence based on the dot product of node embeddings extracted by the encoder: $\text{logits}_{ij} = (z_i \cdot z_j)$, where $z_i$ and $z_j$ are the embeddings of node $i$ and $j$, respectively.
	
	\subsection*{\texttt{DeepGATBlockV1} Architecture}
	
	The \texttt{DeepGATBlockV1} is a modular block composed of a series of \textit{levels}, each of them processing node embeddings using a combination of graph attention mechanism, feed-forward network and normalization technique.
	Each level consists of the following operations:
	\begin{enumerate}
		\item A multi-head graph attention layer, $GAT$, which attends to the features of neighboring nodes, implemented with \texttt{torch\_geometric.nn.GATConv} module.
		The output of the multi-head attention is not concatenated but rather averaged to match the input dimension.
		Given a node feature matrix $X \in \mathbb{R}^{N \times H}$ and an edge index $E$, the GAT layer calculates node representation according to the Graph Attention mechanism: $X_{att} = GAT(X, E)$, where the result $X_{att}$ is in $\mathbb{R}^{N \times H}$.
		\item An attention dropout layer to regularize attention output.
		\item A residual connection between input features and the output of the graph attention layer, followed by a layer normalization. The norm operation can be expressed as: $X_1 = LayerNorm(X + Dropout(X_{att}))$.
		\item A two-layer feed-forward network, $FFN$, each of which apply a linear transformation, non-linear ReLU activation (in between linear transformation), and dropout layer on the output. Given an input features $X_1$, the network can be formalized as: $X_{ff1} =  ReLU(L_1(X_1))$, and $X_{ff2} = Dropout(L_2(X_{ff1}))$. Where $L_1 : \mathbb{R}^{H} \to \mathbb{R}^{H}$ and $L_2 : \mathbb{R}^{H} \to \mathbb{R}^{H}$ represent the linear transformation in the two layer FFN block.
		\item A residual connection between the input features of feed-forward networks, $X_1$ and it's output, $X_{ff2}$, followed by a layer normalization. The norm operation can be expressed as: $X_{out} = LayerNorm(X_1 + X_{ff2})$.
	\end{enumerate}
	The final output of the \texttt{DeepGATBlockV1} is further processed by a dropout to regularize it.
	\paragraph*{Mathematical Representation:}
	Given a number of levels $L$:
	\begin{enumerate}
		\item for each level $l=1, \dots, L$:
		\begin{enumerate}
			\item $X_{att}^l = GAT(X^{l-1}, E)$
			\item $X_1^l = LayerNorm(X^{l-1} + Dropout(X_{att}^l))$
			\item $X_{ff1}^l = ReLU(L_1^l(X_1^l))$
			\item $X_{ff2}^l = Dropout(L_2^l(X_{ff1}^l))$
			\item $X^l = LayerNorm(X_1^l + X_{ff2}^l)$
		\end{enumerate}
		\item Final output: $X_{out} = Dropout(X^L)$
	\end{enumerate}
	
	\subsection*{Graphical Representation}
	To give a graphical perspective of the network architecture we present the following diagram:
	
	
\end{document}